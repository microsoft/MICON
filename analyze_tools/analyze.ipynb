{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d788a13a-a6d2-4350-afcd-24322215df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from numpy import random\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import colorcet as cc\n",
    "import seaborn as sns\n",
    "import numba\n",
    "import time\n",
    "sns.set_style(style='white') \n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "palette = sns.color_palette(\"bright\", 15)\n",
    "palette2 = sns.color_palette(\"dark\", 10)\n",
    "palette3 = sns.color_palette(cc.glasbey, n_colors=20)\n",
    "\n",
    "from umap.umap_ import UMAP\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from process import read_file_embeddings, plate_wise_spherize_and_normailize, average_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82650e6-6af5-4c75-b0da-c82c8b5c9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_embeddings(cpfname, fname, f_dim=512, feature_cols=\"micon_\", is_moa=False):\n",
    "    if cpfname.split('.')[-1] == 'parquet':\n",
    "        df_check = pd.read_parquet(cpfname)\n",
    "    elif cpfname.split('.')[-1] == 'csv':\n",
    "        df_check = pd.read_csv(cpfname, low_memory=False)\n",
    "    df_check[\"Metadata_Fov\"] = df_check[\"Metadata_Fov\"].astype(int)\n",
    "    if is_moa:\n",
    "        df_check[\"Metadata_Moa\"] = df_check[\"Metadata_InChIKey\"].apply(lambda x: get_moa(x)).tolist()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        emb, fname = pkl.load(f)\n",
    "\n",
    "    f_name = []\n",
    "    for x in fname:\n",
    "        [f_name.extend([t.split(\"$\")]) for t in x]\n",
    "\n",
    "    df_emb = pd.DataFrame({\"Metadata_Plate\": [x[2] for x in f_name], \"Metadata_Well\": [x[3] for x in f_name], \"Metadata_Fov\": [int(x[4]) for x in f_name]})\n",
    "    df_feat = pd.DataFrame(data=emb, columns=[f\"{feature_cols}{i}\" for i in range(f_dim)])\n",
    "    df_emb = pd.concat([df_emb, df_feat], axis=1)\n",
    "    df_check = df_check.merge(df_emb, on=[\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Fov\"])\n",
    "    return df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2061a5-da73-4187-a597-aec232bfce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMI2LABEL = {'c1ccc(-c2nn3c(c2-c2ccnc4cc(OCCN5CCOCC5)ccc24)CCC3)nc1': 1,\n",
    " 'COc1ncc2cc(C(=O)Nc3cc(C(O)=NCc4cccc(Cl)c4)ccc3Cl)c(O)nc2n1': 2,\n",
    " 'CC1CC2C3CC=C4CC(=O)C=CC4(C)C3(F)C(O)CC2(C)C1(O)C(=O)CO': 3,\n",
    " 'C=CC1CN2CCC1CC2C(O)c1ccnc2ccc(OC)cc12': 4,\n",
    " 'CCOC(=O)C1OC1C(O)=NC(CC(C)C)C(O)=NCCC(C)C': 5,\n",
    " 'Cc1csc(-c2nnc(Nc3ccc(Oc4ncccc4-c4cc[nH]c(=N)n4)cc3)c3ccccc23)c1': 6,\n",
    " 'O=C(c1ccccc1)N1CCC(CCCCN=C(O)C=Cc2cccnc2)CC1': 7,\n",
    " 'CC(C)N=C(O)N1CCC(N=C2Nc3cc(F)ccc3N(CC(F)F)c3ccc(Cl)cc32)C1': 8,\n",
    " 'CS(C)=O': 'control'}\n",
    "SOURCE_LIST = ['source_2',  'source_3', 'source_5', 'source_6', 'source_7', 'source_8', 'source_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd015f1e-3196-4444-b4f8-8b4c30ba403a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_source_split(X, Y, Y_s, Y_b, source):\n",
    "    X_train, Y_train, Y_s_train, Y_b_train = X[Y_s != source], Y[Y_s != source], Y_s[Y_s != source], Y_b[Y_s != source]\n",
    "    X_test, Y_test, Y_s_test, Y_b_test = X[Y_s == source], Y[Y_s == source], Y_s[Y_s == source], Y_b[Y_s == source]\n",
    "    \n",
    "    return X_train, X_test, (Y_train, Y_s_train, Y_b_train), (Y_test, Y_s_test, Y_b_test)\n",
    "\n",
    "def knn_classifier(df_train, df_test=None, n_neighbors=3, feature_col=[\"Emb_\"], feature_col_test=\"Emb_\", label_col=\"Metadata_Moa\", label_col_test=\"Metadata_Moa\", test_size=0.1):\n",
    "    df_train.dropna(subset=label_col, inplace=True)\n",
    "    X = df_train[feature_col]\n",
    "    y = df_train[label_col].to_numpy().astype('str')\n",
    "    if df_test is not None:\n",
    "        df_test.dropna(subset=label_col_test, inplace=True)\n",
    "        X_test = df_test[feature_col_test]\n",
    "        y_test = df_test[label_col_test].to_numpy().astype('str')\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    print(f\"Training samples: {len(X_train)}. Testing samples: {len(X_test)}\")\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n_neighbors, metric = \"cosine\")\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test).reshape(-1, 1)\n",
    "    v = [accuracy_score(y_test[y_test == i], y_pred[y_test == i]) for i in sorted(np.unique(y_test))]\n",
    "    v += [accuracy_score(y_test, y_pred)]\n",
    "    acc = pd.DataFrame({\"Class\": sorted(np.unique(y_test)) + [\"Total\"], \"# of Samples\": [len(y_test[y_test == i]) for i in sorted(np.unique(y_test))] + [len(y_test)], f\"Acc_Neighbor={n_neighbors}\": v}, index=None)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb75454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_bad_columns(df, cols=None):\n",
    "    if not cols:\n",
    "        cols = [c for c in df.columns if \"Metadata_\" not in c]\n",
    "    stdev = [df[c].std() for c in cols]\n",
    "\n",
    "    cols_to_drop = []\n",
    "    cols_to_drop.extend([cols[i] for i, s in enumerate(stdev) if s < 0.1 or s > 5])\n",
    "    cols_to_drop.extend([c for c in cols if \"Nuclei_Correlation_RWC\" in c])\n",
    "    cols_to_drop.extend([c for c in cols if \"Nuclei_Correlation_Manders\" in c])\n",
    "    cols_to_drop.extend([c for c in cols if \"Nuclei_Granularity_14\" in c])\n",
    "    cols_to_drop.extend([c for c in cols if \"Nuclei_Granularity_15\" in c])\n",
    "    cols_to_drop.extend([c for c in cols if \"Nuclei_Granularity_16\" in c])\n",
    "\n",
    "    df = df[[c for c in df.columns if c not in cols_to_drop]]\n",
    "    return df, cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff41657e-e23a-4175-b23b-4ca80abef38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visualization_two(df_check_1, df_check_2, feature_cols_1, feature_cols_2, hues=None, type_viz='tsne', \\\n",
    "                               perplexity=30, model_1_name=\"Default\", model_2_name=\"Default_2\", n_color=12, plot='all'):\n",
    "    if isinstance(feature_cols_1, str):\n",
    "        cols_1 = [c for c in df_check_1.columns if c.startswith(feature_cols_1)]\n",
    "        cols_2 = [c for c in df_check_2.columns if c.startswith(feature_cols_2)]\n",
    "    else:\n",
    "        cols_1 = feature_cols_1\n",
    "        cols_2 = feature_cols_2\n",
    "    X_1 = df_check_1[cols_1].values\n",
    "    X_2 = df_check_2[cols_2].values\n",
    "    X_1_length = len(X_1)\n",
    "    X = np.concatenate([X_1, X_2])\n",
    "    total_number_of_hue = len(hues)\n",
    "    if total_number_of_hue == 1:\n",
    "        hue_1 = df_check_1[hues[0]]\n",
    "        hue_2 = df_check_2[hues[0]]\n",
    "        Y_1_total = [hue_1.to_numpy()]\n",
    "        Y_2_total = [hue_2.to_numpy()]\n",
    "        # Drop np.nan in hue order\n",
    "        hue_1_order = [sorted(list(set(hue_1[~pd.isna(hue_1)].to_numpy().flatten())))]\n",
    "        hue_2_order = [sorted(list(set(hue_2[~pd.isna(hue_2)].to_numpy().flatten())))]\n",
    "    else:\n",
    "        Y_1_total = [df_check_1[hue] for hue in hues]\n",
    "        Y_2_total = [df_check_2[hue] for hue in hues]        \n",
    "        hue_1_orders = [sorted(list(set(labels[~pd.isna(labels)].flatten()))) for labels in Y_1_total]\n",
    "        hue_2_orders = [sorted(list(set(labels[~pd.isna(labels)].flatten()))) for labels in Y_2_total]\n",
    "        \n",
    "    if type_viz == 'tsne':\n",
    "        tsne = TSNE(perplexity=perplexity)\n",
    "        X_embedded = tsne.fit_transform(X)\n",
    "    elif type_viz == 'umap':\n",
    "        reducer = UMAP(n_neighbors=perplexity)\n",
    "        X_embedded = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise Exception(\"{type_viz} not supported, choose from tsne/umap.\")\n",
    "        \n",
    "    X_1_embedded = X_embedded[:X_1_length]\n",
    "    X_2_embedded = X_embedded[X_1_length:]\n",
    "    \n",
    "    custom_palette1 = sns.color_palette(\"bright\", 15)\n",
    "    custom_palette2 = sns.color_palette(\"dark\", 15)\n",
    "    sns.set_style(style='white') \n",
    "    for hue_name, label_1, label_2, hue_1, hue_2 in zip(hues, Y_1_total, Y_2_total, hue_1_order, hue_2_order):\n",
    "        plt.figure()\n",
    "        if plot == 'all':\n",
    "            sns.scatterplot(x = X_1_embedded[:,0], y = X_1_embedded[:,1], hue=label_1, hue_order=hue_1, legend='full', palette=custom_palette1, markers = ['o'])\n",
    "            sns.scatterplot(x = X_2_embedded[:,0], y = X_2_embedded[:,1], hue=label_2, hue_order=hue_2, style=label_2, legend='full', palette=custom_palette2, markers = ['^']*len(hue_2))\n",
    "            plt.title(f\"{model_1_name}_vs_{model_2_name}_by_{hue_name}\", loc='center')\n",
    "            plt.legend(bbox_to_anchor=(1, 1), loc=2, ncol=4)\n",
    "        elif plot == 'one':\n",
    "            sns.scatterplot(x = X_1_embedded[:,0], y = X_1_embedded[:,1], hue=label_1, hue_order=hue_1, legend='full', palette=custom_palette1, markers = ['o'])\n",
    "            plt.title(f\"{model_1_name}_by_{hue_name}\", loc='center')\n",
    "            plt.legend(bbox_to_anchor=(1, 1), loc=2, ncol=4)\n",
    "        elif plot == 'two':\n",
    "            sns.scatterplot(x = X_2_embedded[:,0], y = X_2_embedded[:,1], hue=label_2, hue_order=hue_2, legend='full', palette=custom_palette2, markers = ['o'])\n",
    "            plt.title(f\"{model_2_name}_by_{hue_name}\", loc='center')\n",
    "            plt.legend(bbox_to_anchor=(1, 1), loc=2, ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcf613b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NS_metric(df, feature_col, on=\"Metadata_Plate\", topk=10, is_generated=False, all_negative=True):\n",
    "    np_features = df[feature_col].to_numpy()\n",
    "    knn = NearestNeighbors(n_neighbors=len(df), metric = \"cosine\")\n",
    "    knn.fit(np_features)\n",
    "    neighbours_mat = knn.kneighbors(np_features, return_distance=False)\n",
    "    CONTROL = 'CS(C)=O'\n",
    "    smiles = df[\"Metadata_SMILES\"].to_numpy()\n",
    "    res = []\n",
    "    if on == \"Metadata_Plate\":\n",
    "        plates = df[\"Metadata_Plate\"].to_numpy()\n",
    "        for i, (s, p, rank) in tqdm(enumerate(zip(smiles, plates, neighbours_mat))):\n",
    "            if is_generated and i >= len(df)/2:\n",
    "                break\n",
    "            if all_negative:\n",
    "                not_same_index = np.argwhere((plates!=p) | ((plates==p) & (smiles!=s)))\n",
    "            else:\n",
    "                not_same_index = np.argwhere(plates!=p)\n",
    "            true_rank = [r for r in rank if r in not_same_index]\n",
    "            if is_generated:\n",
    "                true_rank = [r for r in true_rank if r >= len(df)/2]\n",
    "            true_label = [smiles[r] == s for r in true_rank[:topk]]\n",
    "            res.append(true_label)\n",
    "    elif on == \"Metadata_Source\":\n",
    "        sources = df[\"Metadata_Source\"].to_numpy()\n",
    "        for i, (s, p, rank) in tqdm(enumerate(zip(smiles, sources, neighbours_mat))):\n",
    "            if is_generated and i >= len(df)/2:\n",
    "                break\n",
    "            if all_negative:\n",
    "                not_same_index = np.argwhere((sources!=p) | ((sources==p) & (smiles!=s)))\n",
    "            else:\n",
    "                not_same_index = np.argwhere(sources!=p)\n",
    "            true_rank = [r for r in rank if r in not_same_index]\n",
    "            if is_generated:\n",
    "                true_rank = [r for r in true_rank if r >= len(df)/2]\n",
    "            true_label = [smiles[r] == s for r in true_rank[:topk]]\n",
    "            res.append(true_label)\n",
    "    ranking = np.array(res)\n",
    "    control_index = [i for i in range(len(ranking)) if smiles[i] == CONTROL]\n",
    "    treated_index = [i for i in range(len(ranking)) if smiles[i] != CONTROL]\n",
    "    control_ranking = np.take(ranking, control_index, axis=0)\n",
    "    treated_ranking = np.take(ranking, treated_index, axis=0)\n",
    "    treated_smiles = np.take(smiles, treated_index, axis=0)\n",
    "    def calc_acc(ranking, topk):\n",
    "        acc_stats = []\n",
    "        correct_smiles = []\n",
    "        for i in range(1, topk+1):\n",
    "            acc = 0\n",
    "            cnt = 0\n",
    "            correct_smiles.append(defaultdict(lambda: 0))\n",
    "            for j in range(len(ranking)):\n",
    "                if True in ranking[j,:i]:\n",
    "                    acc += 1\n",
    "                    correct_smiles[i-1][treated_smiles[j]] += 1\n",
    "            acc_stats.append(acc/len(ranking))\n",
    "        return (acc_stats, correct_smiles)\n",
    "    \n",
    "    return calc_acc(treated_ranking, topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382085f0-43f4-4051-9765-38e198693423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NS_metric_across(subject, candidate, feature_col, on=\"Metadata_Plate\", topk=10, all_negative=False, return_smiles=False):\n",
    "    subject_features = subject[feature_col].to_numpy()\n",
    "    candidate_features = candidate[feature_col].to_numpy()\n",
    "    labels = candidate[\"Metadata_SMILES\"].to_list()\n",
    "    total_feature = np.concatenate([subject_features, candidate_features], axis=0)\n",
    "    candidate_index_to_class = dict(zip(range(len(candidate_features)), labels))\n",
    "    nneighbor = NearestNeighbors(n_neighbors=len(candidate_features), metric = \"cosine\")\n",
    "    nneighbor.fit(candidate_features)\n",
    "    subject_mat = nneighbor.kneighbors(subject_features, return_distance=False)\n",
    "    subject_smiles = subject[\"Metadata_SMILES\"]\n",
    "    CONTROL = 'CS(C)=O'\n",
    "    prediction = []\n",
    "    if on == \"Metadata_Plate\":\n",
    "        plates = subject[\"Metadata_Plate\"].to_numpy()\n",
    "        for i, (s, p, rank) in tqdm(enumerate(zip(subject_smiles, plates, subject_mat))):\n",
    "            exclude_index = np.argwhere(plates!=p)\n",
    "            if all_negative:\n",
    "                exclude_index = np.argwhere((plates!=p) | ((plates==p) & (subject_smiles!=s)))\n",
    "            true_rank = [candidate_index_to_class[r] for r in rank if r in exclude_index]\n",
    "            TF_labels = [smi == s for smi in true_rank[:topk]]\n",
    "            prediction.append(TF_labels)\n",
    "    elif on == \"Metadata_Batch\":\n",
    "        batches = subject[\"Metadata_Batch\"].to_numpy()\n",
    "        for i, (s, p, rank) in tqdm(enumerate(zip(subject_smiles, batches, subject_mat))):\n",
    "            exclude_index = np.argwhere(batches!=p)\n",
    "            if all_negative:\n",
    "                exclude_index = np.argwhere((batches!=p) | ((batches==p) & (subject_smiles!=s)))\n",
    "            true_rank = [candidate_index_to_class[r] for r in rank if r in exclude_index]\n",
    "            TF_labels = [smi == s for smi in true_rank[:topk]]\n",
    "            prediction.append(TF_labels)\n",
    "    elif on == \"Metadata_Source\":\n",
    "        sources = subject[\"Metadata_Source\"].to_numpy()\n",
    "        for i, (s, p, rank) in tqdm(enumerate(zip(subject_smiles, sources, subject_mat))):\n",
    "            exclude_index = np.argwhere(sources!=p)\n",
    "            if all_negative:\n",
    "                exclude_index = np.argwhere((sources!=p) | ((sources==p) & (subject_smiles!=s)))\n",
    "            true_rank = [candidate_index_to_class[r] for r in rank if r in exclude_index]\n",
    "            TF_labels = [smi == s for smi in true_rank[:topk]]\n",
    "            prediction.append(TF_labels)\n",
    "    prediction = np.array(prediction)\n",
    "    subject_smi = subject[\"Metadata_SMILES\"].to_numpy()\n",
    "    control_index = [i for i in range(len(subject_smi)) if subject_smi[i] == CONTROL]\n",
    "    treated_index = [i for i in range(len(subject_smi)) if subject_smi[i] != CONTROL]\n",
    "    control_ranking = np.take(prediction, control_index, axis=0)\n",
    "    treated_ranking = np.take(prediction, treated_index, axis=0)\n",
    "    treated_smiles = np.take(subject_smi, treated_index, axis=0)\n",
    "    def calc_acc(ranking, topk, return_smiles=return_smiles):\n",
    "        acc_stats = []\n",
    "        correct_smiles = []\n",
    "        for i in range(1, topk+1):\n",
    "            acc = 0\n",
    "            cnt = 0\n",
    "            correct_smiles.append(defaultdict(lambda: 0))\n",
    "            for j in range(len(ranking)):\n",
    "                if True in ranking[j,:i]:\n",
    "                    acc += 1\n",
    "                    correct_smiles[i-1][treated_smiles[j]] += 1\n",
    "            acc_stats.append(acc/len(ranking))\n",
    "        if return_smiles:\n",
    "            return (acc_stats, correct_smiles)\n",
    "        else:\n",
    "            return acc_stats\n",
    "    \n",
    "    return calc_acc(treated_ranking, topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visualization(df_check, feature_cols, hues= None, \n",
    "                           type_viz='tsne', perplexity=30, model_name=\"Default\", n_color=12, legend=True, show_bg=False,\n",
    "                          save_fig=False):\n",
    "    if isinstance(feature_cols, str):\n",
    "        cols = [c for c in df_check.columns if c.startswith(feature_cols)]\n",
    "    else:\n",
    "        cols = feature_cols\n",
    "    X = df_check[cols].values\n",
    "    total_number_of_hue = len(hues)\n",
    "    if total_number_of_hue == 1:\n",
    "        Y_total = [df_check[hues[0]].to_numpy()]\n",
    "    else:\n",
    "        Y_total = [df_check[hue].to_numpy() for hue in hues]\n",
    "    \n",
    "    if type_viz == 'tsne':\n",
    "        tsne = TSNE(perplexity=perplexity)\n",
    "        X_embedded = tsne.fit_transform(X)\n",
    "    elif type_viz == 'umap':\n",
    "        reducer = UMAP(n_neighbors=perplexity)\n",
    "        X_embedded = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise Exception(\"{type_viz} not supported, choose from tsne/umap.\")\n",
    "        \n",
    "    custom_palette = sns.color_palette(cc.glasbey, n_color)\n",
    "    sns.set_style(style='white') \n",
    "    for hue_name, hue in zip(hues, Y_total):\n",
    "        plt.figure()\n",
    "        hue = [hue[i] if not pd.isnull(Y_total[0][i]) else np.nan for i in range(len(hue))]\n",
    "        bg_index = [x for x in range(len(hue)) if hue[x] == 'bg']\n",
    "        value_index = [x for x in range(len(hue)) if x not in bg_index]\n",
    "        if legend:\n",
    "            sns.scatterplot(x = X_embedded[value_index,0], y = X_embedded[value_index,1], hue=[hue[i] for i in value_index], palette=custom_palette, zorder=5)\n",
    "            if bg_index and show_bg:\n",
    "                sns.scatterplot(x = X_embedded[bg_index,0], y = X_embedded[bg_index,1], hue=[hue[i] for i in bg_index], color=\"lightgray\", zorder=0)\n",
    "        else:\n",
    "            sns.scatterplot(x = X_embedded[value_index,0], y = X_embedded[value_index,1], hue=[hue[i] for i in value_index], palette=custom_palette, legend=None, zorder=5)\n",
    "            if bg_index and show_bg:\n",
    "                sns.scatterplot(x = X_embedded[bg_index,0], y = X_embedded[bg_index,1], hue=[hue[i] for i in bg_index], color=\"lightgray\", legend=None, zorder=0)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        # plt.title(f\"{model_name}_by_{hue_name}\", loc='center')\n",
    "        plt.legend(bbox_to_anchor=(1, 1), loc=2, ncol=1)\n",
    "        if save_fig:\n",
    "            plt.savefig(f\"vis_{hue_name}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f340c-0d6c-406f-9a41-8e224c4cc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"embeddings/target2.centered.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24c10d99-85e4-4c05-9f80-e1553acdd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"embeddings/target2.centered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf4e7f49-dda1-4e20-8194-23c73e2b0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_csv = pd.read_csv(\"datasets/treated_moa_target2/metadata_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56671ac7-dcf1-48ea-a119-2acc2f4ed116",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_cols = [c for c in data.columns if not c.startswith(\"Metadata_\") and not c.startswith(\"micon_\") and not c.endswith(\"_path\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40ce527-814c-408c-9d09-c470875eaeaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35136it [05:20, 109.58it/s]\n"
     ]
    }
   ],
   "source": [
    "cp_fname = \"embeddings/pos_control.centered.parquet\"\n",
    "model_fname = \"embeddings/pos_control_raw_embeddings_supcon_freeze_img_train_14000.pkl\"\n",
    "\n",
    "pos_control = read_file_embeddings(cp_fname, model_fname, f_dim=1000, feature_cols=\"micon_\")\n",
    "cp_cols = [c for c in pos_control.columns if not c.startswith(\"Metadata_\") and not c.startswith(\"micon_\") and not c.endswith(\"_path\")]\n",
    "micon_cols = [c for c in pos_control.columns if c.startswith(\"micon_\")]\n",
    "pos_control = average_wells(pos_control, feature_cols=\"micon_\")\n",
    "\n",
    "# pos_control_processed = plate_wise_spherize_and_normailize(pos_control, plate_col=\"Metadata_Batch\", feature_cols=cp_cols, control_only=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f909c75-7135-452b-9be4-a2cd25839b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 68/72 [02:27<00:07,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No control samples found. Fall back to full normailization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [02:35<00:00,  2.15s/it]\n",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 68/72 [01:51<00:05,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No control samples found. Fall back to full normailization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [01:57<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc98e40f-6e18-47c0-bf74-51ce5b437c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:22<00:00,  3.69s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:14<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "pos_control_processed_source = plate_wise_spherize_and_normailize(pos_control, plate_col=\"Metadata_Source\", feature_cols=cp_cols, control_only=True)\n",
    "pos_control_processed_source = plate_wise_spherize_and_normailize(pos_control_processed_source, plate_col=\"Metadata_Source\", feature_cols=micon_cols, control_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c56d544-f023-4a56-9db6-c720ebb64276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62162it [54:43, 18.93it/s]\n"
     ]
    }
   ],
   "source": [
    "cpfname = \"embeddings/target2.centered.csv\"\n",
    "fname = \"check_embeddings/treated_moa_target2/new_micon_divide255_freeze_img_resnet101_with_generate_with_aux_18000_original.pkl\"\n",
    "\n",
    "target2_freeze_with_all = read_file_embeddings(cpfname, fname, f_dim=1000, feature_cols=\"micon_\", is_moa=False)\n",
    "target2_freeze_with_all_avg = average_wells(target2_freeze_with_all, feature_cols=\"micon_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1e53af35-da02-480a-9edd-b939fb574c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101/101 [02:04<00:00,  1.23s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101/101 [01:46<00:00,  1.06s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "cp_cols = [c for c in target2_freeze_with_all_avg.columns if not c.startswith(\"Metadata_\") and not c.startswith(\"micon_\") and not c.endswith(\"_path\")]\n",
    "micon_cols = [c for c in target2_freeze_with_all_avg.columns if c.startswith(\"micon_\")]\n",
    "target2_batch_processed = plate_wise_spherize_and_normailize(target2_freeze_with_all_avg, plate_col=\"Metadata_Batch\", feature_cols=cp_cols, control_only=True)\n",
    "target2_batch_processed = plate_wise_spherize_and_normailize(target2_batch_processed, plate_col=\"Metadata_Batch\", feature_cols=micon_cols, control_only=True)\n",
    "target2_source_processed = plate_wise_spherize_and_normailize(target2_freeze_with_all_avg, plate_col=\"Metadata_Source\", feature_cols=cp_cols, control_only=True)\n",
    "target2_source_processed = plate_wise_spherize_and_normailize(target2_source_processed, plate_col=\"Metadata_Source\", feature_cols=micon_cols, control_only=True)\n",
    "# pos_control_processed = plate_wise_spherize_and_normailize(target2_freeze_with_all, plate_col=\"Metadata_Batch\", feature_cols=cp_cols, control_only=True)\n",
    "# pos_control_processed = plate_wise_spherize_and_normailize(pos_control_processed, plate_col=\"Metadata_Batch\", feature_cols=micon_cols, control_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

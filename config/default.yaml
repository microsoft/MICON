defaults:
  - _self_
  # - override hydra/hydra_logging: disabled  
  # - override hydra/job_logging: disabled  

mode: 'generate_embedding'

# Experiemnt args
device: 'cuda'
eval_only: false
predict_only: false
seed: 42

ROOT: "/data/MICON-main"
dataset: "treated_moa_target2"

# Dataset args
data:
  train_path: "${ROOT}/datasets/${dataset}" # Absolute path to dataset
  valid_path: "${ROOT}/datasets/${dataset}" 
  generate_path: "${ROOT}/datasets/${dataset}" 
  train_sample_strategy: 'random' # 'molecule', 'sequential', 'img_only'
  valid_sample_strategy: 'molecule' 
  generate_sample_strategy: 'img_only'
  from_pretrained: True 

# Optimizer args
train:
  optim:
    lr: 5e-3
    weight_decay: 1e-d
    grad_clip: 1.0
  top_k: [1, 3, 5]
  batch_size: 256
  epochs: -1
  steps: 100000
  log_every: 10
  lr_scheduler: "ReduceLROnPlateau"
  save: true
  save_per_steps: 1000
  save_dir: "ckpts"

valid:
  batch_size: 1
  per_steps: 10000
  steps: 20

generate_embedding:
  batch_size: 64
  save_path: "${ROOT}/check_embeddings/${dataset}/" 
  generate: false
  method: raw   # raw/projected

ckpt: 28000
load:
  load_path: "/data/MICON-main/train/${dataset}/run_11\
    /ckpts/model_${ckpt}.pt"

# Contrastive args
micon:
  loss: 'supervised' # Whether to add self-supervised contrastive learning loss
  interleave_image: true # Interleave image channels to fit pre-trained config
  generation_head: false # Whether to add generation head to the model
  generation_double_align: true # Whether to use double alignment
  hidden_size: 1000 # Dimensionality of output of encoders
  projection_dim: 512 # Dimensionality of output layers in projectors
  dropout: 0
  temperature: 1.0 # Temperature hyper-parameter for softmax fuction
  aux: false # Whether to add auxiliary loss 
  aux_weight: 1.0 # auxiliary loss weighting 

img_encoder:
  from_pretrained: true
  finetune: true
  model_type: 'resnet' # ['resnet', 'vit']
  model:
    _target_: torchvision.models.resnet101
    # _target_: torchvision.models.vit_b_16
    weights: 'DEFAULT'
  output_size : 1000 # Final output size of the encoder

mol_encoder:
  model: 'MLP' # ['Unimol', 'MLP', 'Graph']
  mol_size: 2048
  # from_pretrained: false
  # depth: 3  # Number of message passing steps
  # atom_messages: false # Centers messages on atoms instead of on bonds.
  # use_input_features: false
  # atom_descriptors: None
  # atom_descriptors_path: None
  # bond_descriptors: None
  # bond_descriptors_path: None
  # is_atom_bond_targets: false
  # undirected: false # Undirected edges (always sum the two relevant bond vectors). 
  # aggregation: 'mean' # ['mean', 'sum', 'norm']
  # aggregation_norm: 100 # For norm aggregation, number by which to divide summed up atomic features
  # activation: 'ReLU'
  # dropout: 0.1 
  # bias: false # Whether to add bias to linear layers
  # output_size : 1000 # Final output size of the encoder

exp_setting: "MICON_test"

hydra:  
  # output_subdir: null  
  run:  
    dir: ${mode}/${dataset}/${now:%Y-%m-%d_%H-%M-%S}_${micon.loss}_${exp_setting}

    
